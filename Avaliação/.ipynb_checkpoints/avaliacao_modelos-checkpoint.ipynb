{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação de Modelos\n",
    "\n",
    "## 1. Métricas de Avaliação de modelos\n",
    "\n",
    "### 1.1 Métricas de Regressão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE\n",
    "O RMSE (*Root Mean Squared Error*) é uma das métricas mais utilizadas em problemas de regressão. \n",
    "\n",
    "Características:\n",
    "- Uma das vantagens do RMSE é que os erros são inicialmente elevados ao quadrado antes de ser tomada a média, logo existe uma **grande penalidade para erros grandes**.\n",
    "- Conserva a mesma unidade de medida da grandeza $x$.\n",
    "\n",
    "$\\text{RMSE} = \\sqrt{\\frac{\\sum_{i=1}^n (\\hat{y_i} - y_i)^2 }{n}}$\n",
    "\n",
    "**Obs.:** RMSLE (*Root Mean Squared Log Error*)\n",
    "Trata-se de uma versão do RMSE, mas com $log$ aplicado às grandezas. O RMSLE é aproximadamente um erro médio percentual.\n",
    "\n",
    "$\\text{RMSLE} = \\sqrt{\\frac{\\sum_{i=1}^n (log(\\hat{y_i}) - log(y_i))^2 }{n}}$\n",
    "\n",
    "Vantagem:\n",
    "Mais robusto a outliers que o RMSE.\n",
    "Exemplo: $y_i = 100$, $y_i = 1000$, $\\text{RMSE} = (100-1000)^2 = 810.000,0$, porém $\\text{RMSLE} = (log(100)-log(1000))^2 = 5,3019$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE\n",
    "O MAE (*Mean Absolute Error*) é a diferença absoluta entre a variável target e o valor previsto pelo modelo. \n",
    "\n",
    "Características:\n",
    "- Mais **robusto a outliers**\n",
    "- Não penaliza tanto os erros quanto o MSE\n",
    "\n",
    "$\\text{MAE} = \\frac{\\sum_{i=1}^n |\\hat{y_i} - y_i| }{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $R^2$\n",
    "O **coeficiente de determinação** ou $R^2$ é uma outra métrica para a avaliação de modelos de regressão.\n",
    "\n",
    "Característica:\n",
    "- $R^2$ é livre de escalas (não importa os valores, sempre teremos $R^2 \\leq 1$).\n",
    "- Na regressão linear simples: $R^2 = \\text{Cor}(X, Y)$\n",
    "- Na regressão linear múltipla: $R^2 = \\text{Cor}(\\hat{Y}, Y)$\n",
    "- $R^2 \\approx 1$ indica que o modelo consegue **explicar uma grande parte da variância** da variável dependente.\n",
    "\n",
    "$R^2 = 1 - \\frac{\\text{MSE(model)}}{\\text{MSE(baseline)}} = \n",
    "        1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y_i})^2 }{\\sum_{i=1}^n (y_i - \\bar{y_i})^2  } = \n",
    "        1 - \\frac{RSS}{TSS}$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $TSS = \\sum_{i=1}^n (y_i - \\bar{y_i})^2$: variância inicial dos dados\n",
    "- $RSS = \\sum_{i=1}^n (y_i - \\hat{y_i})^2$: variância não explicada pelo modelo\n",
    "- $TSS - RSS$: variância explicada pelo modelo\n",
    "\n",
    "Assim, o $R^2$ é uma proporção da variância total explicada pelo modelo.\n",
    "\n",
    "OBS: $R^2$ sempre aumenta quando mais variáveis são adicionadas ao modelo, mesmo se essas variáveis sejam apenas levemente associadas com a variável dependente. Inclusive, se adicionamos uma variável e ela contribui pouco pro $R^2$, ela deve ser pouco relacionada ao target.\n",
    "\n",
    "Podemos usar então o $R^2$ ajustado, que é definido como:\n",
    "\n",
    "$R_{adj}^2 = 1 - \\frac{(1 - R^2)(n-1)}{n-k-1}$\n",
    "\n",
    "Onde $k$ é o número de variáveis independentes. O $R^2$ ajustado penaliza a introdução de novas variáveis ao modelo.\n",
    "\n",
    "OBS: $R^2$ pode ser negativo se o modelo não é bem adaptado aos dados, ou se houverem muitos outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAPE\n",
    "Mean absolute percentage error expressa a acuracia como um erro percentual.\n",
    "\n",
    "$MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{y_i-\\hat{y_i}}{y_i}$\n",
    "\n",
    "Problemas:\n",
    "- Não pode ser usada para valores com $y_i = 0$.\n",
    "- Coloca uma penalidade maior em erros negativos ($ \\hat(y_i) < hat(y)_i$).\n",
    "- Não é tão simples de otimizar como o RMSLE, por exemplo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Métricas de Classificação\n",
    "\n",
    "Inicialmente, é necessário definir alguns conceitos:\n",
    "\n",
    "- TN = **True Negative**: observações de classe negativa corretamente classificadas pelo modelo;\n",
    "- TP = **True Positive**: observações de classe positiva corretamente classificadas pelo modelo;\n",
    "- FN = **False Negative**: observações de classe positiva incorretamente classificadas pelo modelo;\n",
    "- FP = **False Positive**: observações de classe negativa incorretamente classificadas pelo modelo.\n",
    "\n",
    "Esses valores geralmente são exibidos na forma de uma **matriz de confusão**\n",
    "\n",
    "![matriz_confusao](img/matriz_confusao.png)\n",
    "\n",
    "![matriz_confusao_2](img/matriz_confusao_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy (Acurácia)\n",
    "A **acurácia** é a fração de observações cujas saídas foram corretamente previstas pelo modelo: $\\text{Acurácia} = \\frac{\\text{Número de predições corretas}}{\\text{Total de predições}} = \\frac{TP+TN}{TP+TN+FP+FN}$\n",
    "\n",
    "A medida de acurácia pode ser enganadora. No caso abaixo, por exemplo, vemos que a acurácia do modelo é de 99.9%, o que a princípio parece muito bom. No entanto, se imaginarmos que o valor positivo representa por exemplo uma pessoa que carrega um vírus, fica claro que nesse caso o custo de se ter um Falso Negativo é muito elevado.\n",
    "\n",
    "![accuracy](img/accuracy.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision (Precisão)\n",
    "A **precisão** é a proporção de observações de previsão positiva corretamente classificadas pelo modelo: $\\text{Precisão} = \\frac{TP}{TP+FP}$.\n",
    "\n",
    "A precisão é uma boa métrica quando o **custo de Falsos Positivos é elevado**. Por exemplo, no caso de um modelo para detecção de spams, o usuário pode perder email importantes se a precisão não for boa (muitos emails ordinários classificados como spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall (Revocação)\n",
    "O **recall** é a proporção de observações de classe positiva corretamente identificadas pelo modelo: $\\text{Recall} = \\frac{TP}{TP+FN}$.\n",
    "\n",
    "O recall é uma boa métrica quando o **custo de Falsos Negativos é elevado**. Por exemplo, no caso de um modelo de detecção de fraudes, não é desejável que uma transação fraudulenta seja classificada como não-fraudulenta.\n",
    "\n",
    "![precision_recall](img/precision_recall.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1\n",
    "O **f1-score** é uma combinação das métricas de precisão e recall: $\\text{F1} = 2*\\frac{\\text{Precisão *  Recall}}{\\text{Precisão +  Recall}}$.\n",
    "\n",
    "O F1 pode ser uma boa métrica quando gostaríamos de ter um balanço entre precisão e recall, e existe uma distribuição de classes desigual (grande número de negativos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini\n",
    "O index de Gini é uma métrica utilizada sobretudo em modelos de Árvores de Decisão, e representa a **probabilidade de uma variável ser incorretamente classificada** quando escolhida aleatoriamente.\n",
    "\n",
    "O Gini indica se existe uma classe preponderante em uma subdivisão do espaço de preditores ao treinar uma árvore de decisão. Portanto, pode ser utilizado como métrica a se otimizar em modelos desse tipo.\n",
    "\n",
    "$\\text{Gini} = \\sum_{k=1}^K\\hat{p_{mk}}(1 - \\hat{p_{mk}}) = 1 - \\sum_{k=1}^K \\hat{p_{mk}}^2$\n",
    "\n",
    "Onde $m$ é a região (nó da árvore) e $k$ é a classe.\n",
    "\n",
    "Notar que se em uma região $m$ a probabilidade de um exemplo ser da classe $k$ é $p_{mk}$, um valor **próximo de zero ou de um**, $p_{mk} * (1 - p_{mk})$ será **pequeno**. Ao contrario, se $p_{mk}$ é **próximo de 0.5**,  $p_{mk} * (1 - p_{mk})$ sera relativamente grande.\n",
    "\n",
    "$p_{mk}$ próximo de 0 indica que a classe $k$ não predomina, alguma outra deve predominar.\n",
    "$p_{mk}$ próximo de 1 indica que a classe $k$ predomina, e nenhuma outra predomina.\n",
    "$p_{mk}$ próximo de 0.5 **não traz nenhum indicativo** de que a classe $k$ predomina  (por isso a parcela do Gini para essa classe é alta).\n",
    "\n",
    "Quanto **menor** o valor do Gini, mais puro é o nó de uma árvore de decisão, ou seja, melhor ele divide os dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC e ROC\n",
    "A curva ROC é um gráfico que mostra o quão bem um classificador binário performa a medida em que o valor de threshold de discriminação é variado. \n",
    "\n",
    "O threshold de discriminação é o valor que determina o limite acima do qual uma observação será classificada como positiva, e abaixo do qual a observação será classificada como negativa.\n",
    "\n",
    "Os eixos da curva ROC são tais que:\n",
    "- O eixo x é a **taxa de falsos positivos** = $\\frac{FP}{FP + TN}$\n",
    "- O eixo y é a **taxa de verdadeiros positivos** = $\\frac{TP}{TP + FN}$\n",
    "\n",
    "![roc_curve](img/roc_curve.png)\n",
    "\n",
    "Um ponto em cima da linha pontilhada indica que a **proporção de observações positivas corretamente classificadas** (verdadeiro positivo) é igual a **proporção de observações negativas incorretamente classificadas** (falso positivo). \n",
    "\n",
    "A área embaixo da curva (AUC) do gráfico é útil para comparar diferentes curvas de ROC. Quanto mais próximo de $AUC = 1$, melhor o modelo de classificação.\n",
    "\n",
    "Na figura abaixo, a curva de distribuição vermelha indica a classe positiva e a curva de distribuição verde refere-se a classe negativa. Trata-se de um classificador perfeito, que consegue diferenciar perfeitamente as duas classes,\n",
    "\n",
    "![auc_1](img/auc_1.png)\n",
    "\n",
    "Já na figura abaixo, AUC < 1, e percebe-se que o classificador comete alguns erros (FN e FP são maiores do que zero).\n",
    "\n",
    "![auc_07](img/auc_07.png)\n",
    "\n",
    "Por fim, na figura abaixo temos um classificador aleatório, incapaz de distinguir as duas classes.\n",
    "\n",
    "![auc_05](img/auc_05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KS\n",
    "A métrica de KS (Kolmogorov-Smirnov) é uma medida do grau de separação entre as \n",
    "distribuições positivas e negativas. O score KS é 100 se a divisão entre a classe positiva e negativa é perfeita.\n",
    "\n",
    "$\\text{KS} = \\|\\text{Cumulative % positive} - \\text{Cumulative % negative}\\|$\n",
    "\n",
    "OBS: não está incluso no sklearn.metrics.\n",
    "\n",
    "`\n",
    "from scipy.stats import ks_2samp\n",
    "def ks_stat(y,yhat):\n",
    "    return ks_2samp(yhat[y==1], yhat[y!=1]).statistic`\n",
    "\n",
    "Exemplo: Na tabela abaixo, temos 100000 observações de clientes que podem ou não ter dado churn. \n",
    "- Para cada observação, o modelo de classificação fornece uma probabilidade de y = Churn. \n",
    "- As observações são ordenadas em ordem decrescente de probabilidade\n",
    "- Em seguida, são divididas em deciles (grupos de 10000 observações, no caso). \n",
    "- Para cada decil, calcula-se a porcentagem de observações Churn e Não-Churn, bem como as porcentagem **cumulativas** (soma das porcentagens dos decils anteriores + atual).\n",
    "- Calcula-se a diferença entre a porcentagem de observações Churn e Não-Churn.\n",
    "- A maior diferença define o KS.\n",
    "\n",
    "![ks_table](img/ks_table.png)\n",
    "\n",
    "![ks_chart](img/ks_chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Validação\n",
    "\n",
    "A validação out-of-sample se refere ao uso de dados “novos” (que não foram usados no treino) para avaliar o modelo.\n",
    "\n",
    "O erro de generalização (**out-of-sample error**) é a medida do quão bem um algoritmo consegue prever os valores de dados não vistos anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cross-Validation\n",
    "#### Holdout\n",
    "\n",
    "Divide-se aleatoriamente o conjunto de observações em um “training set” e um “validation set” (“hold-out set”). O modelo é treinado a partir do “training set”, e em seguida é utilizado para prever as saídas referente às observações do “validation set”. \n",
    "\n",
    "Observações:\n",
    "- Esse método pode ser usado quando existe uma grande disponibilidade de dados, caso contrário pode levar a uma alta variância nos resultados.\n",
    "- A estimação do erro de teste pode ser **bastante variável**, dependendo em qual observações são incluídas no “training set” e quais são incluídas no “validation set”\n",
    "- Apenas um subset de observações (aquelas presentes no “training set”) são usadas para treinar o modelo. Como métodos estatísticos tendem a performar pior quando treinados em menos observações, as métricas obtidas provavelmente serão uma **superestimação** do erro de teste obtido quando o modelo é treinado no conjunto de dados completo.\n",
    "\n",
    "![holdout](img/holdout.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leave-one-out\n",
    "Também envolve a divisão de dados em dois subsets. Porém, dessa vez uma única observação é usada como o “validation set”.\n",
    "\n",
    "Observações:\n",
    "- A validação do tipo Leave-one-out é computacionalmente custosa, já que o modelo deve ser treinado $n$ vezes.\n",
    "\n",
    "![leave_one_out](img/leave_one_out.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-fold\n",
    "\n",
    "- O conjunto de dados é dividido aleatoriamente em $k$ grupos(folds) de mesmo tamanho. \n",
    "\n",
    "- Um dos $k$ folds é usado como conjunto de validação\n",
    "\n",
    "- Os outros $k-1$ folds são usados para treinar o modelo\n",
    "\n",
    "O procedimento é repetido $k$ vezes, cada vez usando um fold diferente como conjunto de validação.\n",
    "\n",
    "Ao final, calcula-se a média das métricas calculadas em cada fold, conforme mostrado abaixo:\n",
    "\n",
    "![kfolds](img/kfolds.png)\n",
    "\n",
    "Observações:\n",
    "- A validação do tipo Leave-one-out é um caso específico de k-Fold no qual $k=n$\n",
    "- Geralmente, utiliza-se $k = 5$ ou $k = 10$\n",
    "- A validação k-Fold costuma fornecer estimativas mais precisas da taxa de erro de teste que a validação Leave-one-out\n",
    "\t- Holdout: superestima o erro de teste (alto bias)\n",
    "\t- Leave-one-out: fornece estimativas sem nenhum bias, porém a variância é elevada, uma vez que os modelos treinados são praticamente idênticos (bastante correlacionados). A média de quantidades muito correlacionadas possui uma variância maior do que a média de quantidades não tão correlacionadas.\n",
    "- K-fold: nível intermediário de bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out of time\n",
    "\n",
    "Uma conjunto de validação do tipo **out-of-time** contém dados de um período completamente diferente daquele utilizado para o desenvolvimento do modelo.  \n",
    "\n",
    "Vantagens:\n",
    "- Validar a performance do modelo em um período de tempo diferente é útil para avaliar o quão robusto o modelo é.\n",
    "\n",
    "Desvantagens:\n",
    "- A habilidade de fazer previsões out-of-time não é levada em conta quando o modelo é construído (o modelo selecionado é aquele que tem uma boa performance nos dados de validação in-time)\n",
    "\n",
    "Na figura abaixo, temos um exemplo de validação in-time e teste out-of-time:\n",
    "![out_of_time](img/out_of_time.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra\n",
    "Alguns exercícios da AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1\n",
    "\n",
    "A data scientist is working on optimizing a model during the training process by varying multiple parameters. The data scientist observes that, during multiple runs with identical parameters, the loss function converges to different, yet stable, values. What should the data scientist do to improve the training process?\n",
    "\n",
    "A) Increase the learning rate. Keep the batch size the same.\n",
    "\n",
    "B) Reduce the batch size. Decrease the learning rate.\n",
    "\n",
    "C) Keep the batch size the same. Decrease the learning rate.\n",
    "\n",
    "D) Do not change the learning rate. Increase the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta = B\n",
    "\n",
    "Provavelmente a função tem muitos mínimos locais. Diminuir o batch size ajuda o cientista a sair do mínimo local. Diminuir o learning rate previne o overshoot do mínimo global."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2\n",
    "A data scientist is evaluating different binary classification models. A false positive result is 5 times more expensive (from a business perspective) than a false negative result. The models should be evaluated based on the following criteria:\n",
    "\n",
    "1) Must have a recall rate of at least 80%\n",
    "\n",
    "2) Must have a false positive rate of 10% or less\n",
    "\n",
    "3) Must minimize business costs\n",
    "\n",
    "After creating each binary classification model, the data scientist generates the corresponding confusion matrix.\n",
    "\n",
    "Which confusion matrix represents the model that satisfies the requirements?\n",
    "\n",
    "A) TN = 91, FP = 9/ FN = 22, TP = 78\n",
    "\n",
    "B) TN = 99, FP = 1/ FN = 21, TP = 79\n",
    "\n",
    "C) TN = 96, FP = 4/ FN = 10, TP = 90\n",
    "\n",
    "D) TN = 98, FP = 2/ FN = 18, TP = 82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Alternativa  A\n",
      "Recall: 0.78\n",
      "False positive rate: 0.10344827586206896\n",
      "Business cost: 67\n",
      "\n",
      "# Alternativa  B\n",
      "Recall: 0.79\n",
      "False positive rate: 0.0125\n",
      "Business cost: 26\n",
      "\n",
      "# Alternativa  C\n",
      "Recall: 0.9\n",
      "False positive rate: 0.0425531914893617\n",
      "Business cost: 30\n",
      "\n",
      "# Alternativa  D\n",
      "Recall: 0.82\n",
      "False positive rate: 0.023809523809523808\n",
      "Business cost: 28\n",
      "\n",
      "Resposta: D\n"
     ]
    }
   ],
   "source": [
    "def getMetrics(conf_matrix):\n",
    "    TN = conf_matrix[0]\n",
    "    FP = conf_matrix[1]\n",
    "    FN = conf_matrix[2]\n",
    "    TP = conf_matrix[3]\n",
    "    print(\"Recall:\", TP/(TP+FN))\n",
    "    print(\"False positive rate:\", FP/(TP+FP))\n",
    "    print(\"Business cost:\", 5*FP + FN)\n",
    "    print(\"\")\n",
    "\n",
    "alternativas = {\"A\": [91,9,22,78], \"B\": [99,1,21,79], \"C\": [96,4,10,90], \"D\": [98,2,18,82]}\n",
    "for i in alternativas:\n",
    "    print(\"# Alternativa \",i)\n",
    "    getMetrics(alternativas[i])\n",
    "print(\"Resposta: D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3\n",
    "A data scientist uses logistic regression to build a fraud detection model. While the model accuracy is 99%, 90% of the fraud cases are not detected by the model. What action will definitively help the model detect more than 10% of fraud cases?\n",
    "\n",
    "A) Using undersampling to balance the dataset\n",
    "\n",
    "B) Decreasing the class probability threshold\n",
    "\n",
    "C) Using regularization to reduce overfitting\n",
    "\n",
    "D) Using oversampling to balance the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta = B\n",
    "\n",
    "Diminuir o threshold permite que mais casos sejam marcados como positivos (fraude). OBS: isso irá reduzir a precisão do modelo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação\n",
    "\n",
    "## 1. Regressão Logística\n",
    "- Supervisionada: y = 0 (classe negativa) ou 1 (classe positiva ==> é a que queremos detectar, mais rara)\n",
    "- Paramétrica\n",
    "- Rede neural de uma camada\n",
    "- Feature scaling necessária (gradient descent)\n",
    "\n",
    "Exemplos:\n",
    "- Email: **Spam (1)** x Not-spam (0)\n",
    "- Transações: **Fraude (1)** x Não fraude (0)\n",
    "- Crédito: adimplente (0) x **inadimplente (1)**\n",
    "\n",
    "Por que não Regressão Linear?\n",
    "- Predições > 1 ou < 0 não fazem sentido\n",
    "- Outliers podem afetar o preditor\n",
    "\n",
    "![linear_vs_logistic](img/linear_vs_logistic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Função de predição: Sigmoide\n",
    "Função de predição (modelo) limitada entre 0 e 1:\n",
    "\n",
    "$g(z) = \\frac{1}{1 + e^{-z}}$ com $z = \\theta^Tx$ (sigmoide)\n",
    "\n",
    "Sendo X uma matriz pxn (p: número de parâmetros, n: número de samples) e $\\theta$ um vetor de dimensão p.\n",
    "\n",
    "Uma das vantagens da função sigmoide é que sua derivada é simples de calcular:\n",
    "\n",
    "$s'(z) = s(z)(1 - s(z))$\n",
    "\n",
    "![sigmoide](img/sigmoide.png)\n",
    "\n",
    "$g(z) \\geq 0.5 \\implies z \\geq 0$\n",
    "$g(z) \\leq 0.5 \\implies z \\leq 0$\n",
    "\n",
    "O g(z) é a \"probabilidade de prevermos 1 (a classe positiva). Ou seja, $h_{\\theta}(x) = p(y = 1;x)$.\n",
    "\n",
    "#### Função de custo\n",
    "Não podemos usar a mesma função de custo que a regressão linear, pois isso resultaria em uma função não-convexa. Isso implicaria em vários mínimos locais, de forma que o gradient descent poderia não encontrar o mínimo global.\n",
    "\n",
    "![convex_not_convex](img/convex_not_convex.png)\n",
    "\n",
    "Usaremos então a função Cross-Entropy (Log-Loss):\n",
    "\n",
    "\\begin{equation}\n",
    "cost(h_{\\theta}(x), y) =  \n",
    "\\begin{cases} \n",
    "-log(h_{\\theta}(x))  , &y = 1\\\\\n",
    "-log(1 - h_{\\theta}(x))  , &y = 0\\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Ou de outra forma:\n",
    "\n",
    "$cost(h_{\\theta}(x), y) = -ylog(h_{\\theta}(x)) - (1-y)log(1 - h_{\\theta}(x))$\n",
    "\n",
    "![cost_logistic](img/cost_logistic.png)\n",
    "\n",
    "Quando y=1, queremos penalizar predições próximas de 0\n",
    "\n",
    "Quando y=0, queremos penalizar predições próximas de 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Gradient Descent\n",
    "repeat {\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$\n",
    "\n",
    "} (simultaneously update $\\theta_j$ for $j=0, 1, ..., n$)\n",
    "\n",
    "A derivada da função Log-Loss é proporcional à diferença entre a predição e o valor real (h(x)-y). Se a predição é boa, a diferença é pequena, e portanto a derivada é pequena (estamos nos aproximando do mínimo).\n",
    "\n",
    "#### Decision Boundary\n",
    "$H_{\\theta}(x) \\geq 0.5 \\implies \\theta^Tx \\geq 0 \\implies predict : 1$\n",
    "\n",
    "$H_{\\theta}(x) \\leq 0.5 \\implies \\theta^Tx \\leq 0 \\implies predict : 0$\n",
    "\n",
    "Decision boundary: $\\theta^Tx = 0$ (pois nosso threshold está em 0.5)\n",
    "\n",
    "![decision_boundary](img/decision_boundary.png)\n",
    "\n",
    "Se **diminuímos o threshold** (exemplo: threshold = 0.25), então o intervalo em que prevemos 1 é maior. Logo somos **menos exigentes** na nossa predição.\n",
    "\n",
    "Se **aumentamos o threshold** (exemplo: threshold = 0.75), então o intervalo em que prevemos 1 é menor. Logo somos **mais exigentes** na nossa predição.\n",
    "\n",
    "#### Interpretação dos coeficientes\n",
    "Logit:\n",
    "\n",
    "$ln\\frac{p(1|X)}{1 - p(1|X)} = b_0 + b_1x_1 + ... + b_jx_j$\n",
    "\n",
    "Aumentar X de uma unidade altera as log odds de $\\beta_1$, ou seja, multiplica as odds por $e^{-\\beta_1$.\n",
    "\n",
    "A quantidade que p(x) muda por conta de uma mudança de uma unidade em X depende do valor atual de X.\n",
    "\n",
    "Obs: Odds = $\\frac{p(x)}{1 - p(x)}$\n",
    "\n",
    "A **probabilidade** de um evento acontecer é a **fração de vezes** que você espera ver aquele evento em várias tentativas.\n",
    "\n",
    "As **chances (odds)** são definidas como sendo a **fração entre a probabilidade do evento ocorrido pela probabilidade do evento não ocorrer.**\n",
    "\n",
    "Ex: Se um cavalo de corrida ganha 25 vezes durante 100 corridas, e perde nas 75 restantes, a probabilidade dele ganhar é 0.25, e as chances dele ganhar são 25/75 = 0.333...\n",
    "\n",
    "#### Determinação dos coeficientes\n",
    "A determinação dos coeficientes é feita através do método do Gradient Descent.\n",
    "\n",
    "Max Verossimilhança: a gente consegue descobrir o formato do custo que tenta maximizar as probabilidades dos exemplos\n",
    "\n",
    "A ideia é maximizar a função a seguir:\n",
    "\n",
    "$l(\\beta_0, \\beta_1) = \\prod_{i: y_i = 1}p(x_i)\\prod_{i': y_{i'} = 0}p(x_{i'})$\n",
    "\n",
    "Podemos ver que nos casos onde a probabilidade calculada é próxima do valor real de y, a função de verossimilhança é maximizada.\n",
    "\n",
    "#### Regularização\n",
    "\n",
    "Na regressão logistica, também podemos nos de parar com problemas de overfitting. Para contornar esse problema podemos aplicar técnicas de regularização.\n",
    "\n",
    "![overfit_logistic](img/overfit_logistic.png)\n",
    "\n",
    "A função de custo é atualizada da seguinte forma:\n",
    "\n",
    "FALTA UMA IMAGEM AQUI\n",
    "\n",
    "#### Classificação multiclasse\n",
    "Problemas em que y pode assumir mais de dois valores\n",
    "\n",
    "Ex.: temperatura (y = {ensolarado, chuvoso, nublado), classificar animal em uma imagem (y = {cachorro, gato, peixe, ...})\n",
    "\n",
    "![multiclass](img/multiclass.jpeg)\n",
    "\n",
    "**One Vs All**\n",
    "\n",
    "![one_vs_all](img/one_vs_all.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes\n",
    "\n",
    "- Supervisionado\n",
    "- Paramétrico\n",
    "- Uma coleção de algoritmos (classificadores de Naive Bayes)\n",
    "- Não precisa de feature scaling\n",
    "- Baseia-se no Teorema de Bayes:\n",
    "\n",
    "$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$ ou $posterior = \\frac{likelihood * prior}{evidence}$\n",
    "\n",
    "Assume que:\n",
    "- Cada par de features é **independente** (o que raramente é válido...)\n",
    "\n",
    "Modelo da probabilidade de y, dado um dataset X:\n",
    "$P(y|X) = \\frac{P(X|y)P(y)}{P(X)}\n",
    "\n",
    "Como assumimos que as features são independentes, temos que $P(x_i,x_j) = P(x_i)P(x_j)$. Portanto:\n",
    "\n",
    "$P(y|x_1, ..., x_n) = \\frac{P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)}$\n",
    "\n",
    "O denominador é constante, portanto podemos ignorá-lo:\n",
    "\n",
    "$P(y|x_1, ..., x_n) \\propto P(y)\\prod_{i=1}^nP(x_i|y)$\n",
    "\n",
    "O classificador pode ser escrito como:\n",
    "\n",
    "$\\hat{y} = argmax_{k \\in {1,...,K}} p(C_k)\\prod_{i=1}^n p(x_i|C_k)$\n",
    "\n",
    "O valor calculado é **proporcional às probabilidades** (Nosso dataset pode não ser representativo)\n",
    "\n",
    "Nós **estimamos** as curvas de probabilidade!!! Logo, **não são realmente probabilidades**. São **likelihoods**. Existe uma chance de o valor estimado de probabilidade pelo modelo não ser o valor real da probabilidade.\n",
    "\n",
    "Ex: Detector de spams\n",
    "\n",
    "Em um conjunto de 100 emails, 25 deles são spam.\n",
    "\n",
    "|  | Buy| Cheap | Buy + Cheap |\n",
    "| --- | --- | --- | --- |\n",
    "| Spam | 20 | 15 | 12|\n",
    "| Não-Spam | 5 | 10 | 0 |\n",
    "\n",
    "Se um email tem as palavras Buy e Cheap, é spam ou não spam?\n",
    "P(S|\"Buy\"+\"Cheap) P(S) P(\"Buy\"|S) P[\"Cheap\"|S) = 25/100 * 20/25 * 15/25 = 0.12\n",
    "\n",
    "P(NS|\"Buy\"+\"Cheap) P(NS) * P(\"Buy\"| NS) P(\"Cheap\" |NS) = 75/100 * 5/75 * 10/75 = 0.0067\n",
    "\n",
    "Como P(S|\"Buy\"+\"Cheap) > P(NS|\"Buy\"+\"Cheap) -> é spam\n",
    "\n",
    "Ex:\n",
    "\n",
    "y = {sim, não) ==> jogar golf hoje\n",
    "\n",
    "x1 = {sol, encoberto, chuva) ==> clima\n",
    "\n",
    "x2 = {calor, médio, frio) ==> temperatura\n",
    "\n",
    "![outlook_temp](img/outlook_temp.png)\n",
    "\n",
    "Pra cada valor de variável x1 e xj, estimamos $p(x_i|y_j)$, pra cada $i$ e $j$, construindo a tabela anterior, a partir das ocorrências das variáveis no dataset.\n",
    "\n",
    "Para estimarmos a probabilidade de irmos jogar golf dado que hoje está (X=(sol, calor)):\n",
    "\n",
    "$p(y = Sim | X) \\propto p(x_1 = sol| y = Sim) * p(x_2 = calor | y = Sim) * p(y = Sim)$\n",
    "\n",
    "$p(y = Não | X) \\propto p(x_1 = sol| y = Não) * p(x_2 = calor | y = Não) * p(y = Não)$\n",
    "\n",
    "Vemos qual é maior e decidimos.\n",
    "\n",
    "### Multinomial\n",
    "\n",
    "As variáveis preditoras são valores discretos que correspondem à frequência de ocorrência de cada um dos eventos\n",
    "\n",
    "$X = (x_1, x_2, ..., x_n)$, com cada $x_i$ sendo ocorrências do evento $i$\n",
    "\n",
    "$p = (p_1, p_2, ..., p_n)$ são as probabilidades estimadas de cada evento, com base no dataset\n",
    "\n",
    "Likelihood de observarmos um dado X:\n",
    "\n",
    "$p(x|C_k) = \\frac{(\\sum_i x_i)!}{\\prod_i x_i!}\\prod_i p_{ki}^{x_i}$\n",
    "\n",
    "Usado para Bag of Words: cada $x_i$ é a qtde de uma palavra no texto. Queremos estimar a probabilidade de o texto pertence a alguma classe $y_j$.\n",
    "\n",
    "OBS: Bernouilli-caso particular do multinomial, onde as features são valores booleanos (binários).\n",
    "\n",
    "### Gaussiano\n",
    "As variáveis preditoras são valores continuos, e **assumimos** que elas provêm de uma distribuição Gaussiana.\n",
    "\n",
    "\n",
    "$p(x=v|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}e^{-\\frac{(v-\\mu_k)^2}{2\\sigma_k^2}}$\n",
    "\n",
    "Se algum $x_i$ não é normal, podemos aplicar o método KDE (Kernel Density Distribution): A ideia é convoluir uma Kernel distribution que tente normalizar a estimativa de probabilidades $f(x_i)$.\n",
    "\n",
    "Observações:\n",
    "- E se X teste for muito grande? (algumas probs serão bem pequenas)\n",
    "    - Tirar o log pode resolver o problema do underflow\n",
    "- O classificador Bayesiano é ótimo (matematicamente)\n",
    "    - Mas a hipótese de independência + não representatividade da amostra X em relação à população ==> na realidade não é ótimo.\n",
    "- Não deveria funcionar, pois as vars devem ter uma correlação (parece uma hipótese\n",
    "fraca), mas na prática, empiricamente, vemos que o algoritmo funciona bem!\n",
    "    - O Naive Baves raramente consegue estimar bem a probabilidade correta das classes, mas isso não costuma ser necessário para muitas aplicações. O importante é saber qual classe tem maior probabilidade, e isso ele consegue fazer relativamente bem.\n",
    "- Zero frequency: valor de um $x_i$ (categórico) que não ocorre nunca no train, mas ocorre no teste, terá probabilidade estimada O pelo modelo.\n",
    "    - Alternativa: pode-se atribuir 1 no início para o count desses valores de $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. KNN pra classificação\n",
    "- Supervisionado\n",
    "- Não paramétrico\n",
    "- Preguiçoso → Funciona a partir do cálculo de distâncias (ao dar fit, ele carrega todos os dados na memória) → Se o dataset for mto grande ele vai demorar mais \n",
    "- Precisa de feature scaling (Normalizações)\n",
    "\n",
    "FALTA UMA IMAGEM AQUI\n",
    "\n",
    "- Curse of dimensionality\n",
    "\n",
    "O KNN busca estimar a distribuição condicional de Y dado X, e então classificar uma determinada observação para a classe com maior probabilidade **estimada**. A probabilidade da classe j dado X é dada pela fração de pontos no espaço cujo valor de\n",
    "resposta é j:\n",
    "\n",
    "$P_r(Y=j|X=x_0) = \\frac{1}{K}\\sum_{i \\in N_0}I(y_i=j)$\n",
    "\n",
    "Treino: Somente escolher os pontos que vão fazer parte do espaço.\n",
    "\n",
    "Teste: Usar os pontos anteriores pra fazer a votação (encontrar a classe mais frequente). Atribuímos essa classe ao novo exemplo,\n",
    "\n",
    "### 3.1 Algoritmo\n",
    "O sample é classificado por um voto majoritário de seus K vizinhos.\n",
    "\n",
    "### 3.2 Escolha do k\n",
    "K pequeno: Decision Boundary muuuito flexível (overfit)\n",
    "\n",
    "K grande: Decision Boudary inflexível, usando muitos pontos do dataset pra cada cálculo de média (underfit)\n",
    "\n",
    "![knn_1_100](img/knn_1_100.jpg)\n",
    "\n",
    "Problema binário (duas classes) e valor de K par.\n",
    "\n",
    "E empates, como ele resolve?\n",
    "- Desempate com pesos da distância\n",
    "- Desempate aleatório\n",
    "\n",
    "Se o número de classes é impar (3, por exemplo) e K par (K = 4, por exemplo), não ocorrerão tantos empates.\n",
    "\n",
    "É interessante utilizar K ímpar.\n",
    "\n",
    "\n",
    "### 3.3 Lazy Algorithm\n",
    "- Não constroem descrições gerais e explícitas (função alvo) a partir dos exemplos de treinamento;\n",
    "- Generalização é adiada até o momento da classificação;\n",
    "- Armazena-se uma base de exemplos instances) que é usada para realizar a classificação de uma nova query (exemplo não visto);\n",
    "- Em muitos casos apresenta um alto custo computacional (por conta do cálculo de distâncias).\n",
    "\n",
    "### 3.4 Como lidar com atributos nominais?\n",
    "Mudar a função de distância-e.g., usando coeficiente de casamento simples (simple matching):\n",
    "\n",
    "$d_{SM}(x,y) = \\sum_{i=1}^n s_i$\n",
    "\n",
    "Onde $s_i$ é tal que $(x_i = y_i) \\implies s_i = 0$ e $(x_i \\neq y_i) \\implies s_i = 1$\n",
    "\n",
    "$f(x_q) = argmax_{v \\in V} \\sum_{i=1}^k w_i \\delta(x, f(x_i))$ \n",
    "\n",
    "Onde $\\delta(a,b)$ é tal que $(a = b) \\implies \\delta(a,b) = 1$ e $(a \\neq b) \\implies \\delta(a,b) = 0$\n",
    "\n",
    "![knn_nominal](img/knn_nominal.png)\n",
    "\n",
    "(Hipótese) Usa vars continuas para decidir os k mais próximos, em seguida, verifica nesses k exemplos quantos (count) têm um valor de X K != do próprio valor. Com isso, simula-se uma distância."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Árvores de Classificação\n",
    "- Árvore de Classificação\n",
    "- Supervisionado\n",
    "- Não paramétrico\n",
    "- Não precisa de feature scaling\n",
    "\n",
    "**Vantagens**:\n",
    "- Fácil de compreender/visualizar\n",
    "\n",
    "**Desvantagens**:\n",
    "- Possibilidade de overfitting\n",
    "\n",
    "**Como fazer previsões?**\n",
    "\n",
    "\"Descer\" na árvore até o no correspondente. A saída prevista será a classe que ocorre mais naquela folha.\n",
    "\n",
    "### 4.1 Gini\n",
    "(== impureza de uma região (falta de preponderância de uma classe na região))\n",
    "\n",
    "Para cada classe,\n",
    "\n",
    "$G = \\sum_{k=1}^K \\hat{p_{mk}}(1 - \\hat{p_{mk}})$\n",
    "\n",
    "Outra forma de escrever\n",
    "\n",
    "$G = 1 - \\sum_{k=1}^K \\hat{p_{mk}}^2$\n",
    "\n",
    "Exemplos:\n",
    "- 1-(0.5^2-(0.5)^2 = 0.5\n",
    "- 1-(0.2)^2-(0.8)^2 = 0.32\n",
    "- 1 - (0.05)^2 - 10.95)^2 = 0.095 (a folha tem muitos exemolos de uma classe, a divisão foi muito boa)\n",
    "- 1-(1.0)^2-(0.0)^2 = 0 (divisão perfeita)\n",
    "\n",
    "### 4.2 Cross-Entropy\n",
    "Uma alternativa ao Gini é a cross-entropy\n",
    "\n",
    "$D = -\\sum_{k=1}^K \\hat{p_{mk}}log(\\hat{p_{mk}})$\n",
    "\n",
    "($\\approx 0$ se $p \\approx 1$ ou $p \\approx 0$, como no Gini)\n",
    "\n",
    "### 4.3 Construção da árvore\n",
    "1. Calcular a entropia (ou impureza) da variável target\n",
    "\n",
    "2. O dataset é dividido a partir dos diferentes atributos.\n",
    "\n",
    "    a. A entropia para cada ramo é calculada\n",
    "    \n",
    "    b. Calcula-se a entropia total da divisão (split) a partir de uma média ponderada das entropias de cada ramo.\n",
    "\n",
    "    c. Calcula-se o **Ganho de Informação = entropia antes da divisão - entropia da divisão**\n",
    "    \n",
    "3. Escolher o atributo com maior Ganho de Informação para ser o nó de decisão. Dividir o dataset em dois ramos a partir desse nó.\n",
    "\n",
    "4. Repetir o processo em cada Ramo\n",
    "\n",
    "- Pre poda: gtde de exemplos max por folha, etc.\n",
    "- Pós poda: constrói a árvore e vai cortando ramos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bagging, Boosting, Random Forest\n",
    "\n",
    "Melhoram a predição, mas reduzem a interpretabilidade:\n",
    "\n",
    "FALTA UMA IMAGEM AQUI\n",
    "\n",
    "### 5.1 Bagging (é um método geral) - Boostrae AGgregation\n",
    "Árvores são \"building blocks\".\n",
    "\n",
    "Treinamos B árvores de regressão, usando B bootstrapped training sets. Fazemos a média das predições das B árvores.\n",
    "\n",
    "(Bootstrapped sample: amostra tirada aleatoriamente do dataset original)\n",
    "\n",
    "$\\hat{f}_{bag}(x) = \\frac{1}{B}\\sum_{b=1}^B\\hat{f}^{*b}(x)$\n",
    "\n",
    "As árvores intermediárias não são podadas.\n",
    "\n",
    "Ideia: reduzir a variância! Propriedade: $Var(\\bar{x}) = \\frac{\\sigma^2}{n}$\n",
    "\n",
    "#### Out of Bag Error\n",
    "Quando realizamos o Bootstrap, em torno de 1/3 dos dados não são selecionados. Esses dados compõem o Out-of-Bag dataset, que pode ser utilizado para validar o nosso modelo. Assim, aplicamos o modelo no Out-of-Bag dataset, e a proporção de observações que forem incorretamente classificadas definirão o **Out-of-Bag Error**.\n",
    "\n",
    "\n",
    "#### Feature importance\n",
    "Somar o quanto que o Gini Index diminui a cada divisão em relação a uma feature, e fazer a média de todas as árvores B. Com isso, conseguimos determinar as features mais importantes.\n",
    "\n",
    "### 5.2 Random Forest\n",
    "- Bagged Tree melhorado\n",
    "- Estratégia para descorrelacionar as árvores\n",
    "\n",
    "Treinamos B árvores de regressão, usando B bootstrapped training sets. Fazemos a média das predições das B árvores.\n",
    "\n",
    "A diferença é que, sempre que formos fazer um split na construção de uma árvore, vamos **escolher aleatoriamente $m \\leq \\sqrt{p}$ preditores de todos os $p$ preditores do modelo**.\n",
    "\n",
    "Então, usamos esses $m$ preditores para calcular o melhor corte.\n",
    "\n",
    "Ideia: Preditores importantes ficam no topo da maioria das árvores, no bagging. Assim, evitamos isso na RF, e descorrelacionamos as árvores.\n",
    "\n",
    "B pode ser tão grande quanto quisermos, não dá overfitting (RF, Bagging).\n",
    "\n",
    "\n",
    "#### Algoritmo\n",
    "Considere que temos N exemplos de treinamento\n",
    "\n",
    "Para cada uma das t iterações faça:\n",
    "\n",
    "1. Amostrar N exemplos com reposição (bagging).\n",
    "\n",
    "2. Induzir uma árvore repetindo recursivamente os seguintes passos em cada novo nó da árvore, até que o critério de parada seja satisfeito:\n",
    "\n",
    "    a. Selecionar m atributos (e.g., $m \\leq \\sqrt{p}$) aleatoriamente.\n",
    "\n",
    "    b. Selecionar o melhor atributo/corte (entre os m candidatos).\n",
    "\n",
    "    c. Criar dois nós filhos usando o ponto de corte do passo anterior.\n",
    "3. Armazenar a árvore obtida.\n",
    "\n",
    "Para cada uma das t árvores classificação:\n",
    "1. Predizer o rótulo de classe do exemplo do conjunto-alvo.\n",
    "\n",
    "2. Retornar a classe predita com maior frequência.\n",
    "\n",
    "#### Parámetros Sklearn\n",
    "- n estimators (B): número de árvores na floresta\n",
    "    - Quanto mais árvores, menor a probabilidade de dar overfit\n",
    "    - N estimators = 1 equivale a uma árvore de classificação\n",
    "    -Ex: 100, 200,500...\n",
    "- max depth: máxima profundidade das árvores\n",
    "    - Quanto menor, mais simples serão as árvores, diminuindo o overfit\n",
    "    - Ex: 5,10,20...\n",
    "- min samples leaf: número mínimo de exemplos para ser considerado uma folha\n",
    "    - Ex: 1,2,4..\n",
    "- max features (m): número de features para considerar quando estiver procurando a melhor divisão\n",
    "    - O mais comum é $m = \\sqrt{p}\n",
    "    - Quanto menor, menor a probabilidade de dar overfit (mas se for muito pequeno pode dar underfit)\n",
    "    - Ex: 'auto', 'sgrt', 'log2'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

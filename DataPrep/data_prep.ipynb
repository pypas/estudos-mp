{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataPrep\n",
    "\n",
    "Data Preparation é o processo de limpeza e transformação de dados “brutos” antes de realizar análises e criar modelos. \n",
    "\n",
    "## 1. Tratamento de Missings\n",
    "Muito comum na fase de EDA (Exploratory Data Analysis), precisamos tratar **dados faltantes** no dataset.\n",
    "\n",
    "As principais razões para termos dados faltantes (_missing values_) são:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Missing at Random (MAR):** A propensão para um dado estar faltando não está relacionado à falta do dado em si, mas sim a algum dado observado. A probabilidade de o dado ser faltante é a mesma dentro de cada subgrupo de **dados observados**.\n",
    "    - Exemplo: Estimar a renda bruta anual de uma residência em uma população, a partir de questionários. No caso do MAR, a falta de dados é aleatória em subgrupos de outras variáveis observadas. Por exemplo, suponha que você coletou dados da profissão de cada indivíduo no questionário, e deduziu que gerentes, VIPs, etc. são mais propensos a não compartilhar suas receitas. Então, dentro desses subgrupos de profissões, a falta de dados é aleatória.\n",
    "    - Essa hipótese é mais geral e realista que o MCAR. Os métodos para lidar com missing data atuais normalmente partem da hipótese do MAR.\n",
    "\n",
    "- **Missing Completely at Random (MCAR):** A probabilidade de o dado ser faltante é a mesma em todos os casos, e não estão relacionadas aos dados em si.\n",
    "- Exemplo: Estimar a renda bruta anual de uma residência em uma população, a partir de questionários. No caso do MCAR, a falta de dados é completamente aleatória, como se alguns questionários fossem perdidos.\n",
    "\n",
    "- **Missing Not at Random (MNAR):** Os valores faltantes dependem de outras variáveis, e a probabilidade de os valores estarem faltando são desconhecidas. Em geral, são situações difíceis de reconhecer e tratar.\n",
    "- Exemplo: Pessoas não querem revelar suas rendas, porque elas têm vergonha dela.\n",
    "- Estratégias para achar dados sobre as causas de falta são: encontrar mais dados sobre as causas de falta, fazer análise what-if para ver como os dados respondem em vários cenários.\n",
    "- Possibilidade de se escalar a coleta, produzirmos mais dados faltantes (já que depende de variáveis do problema).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos casos de **falta aleatória**, podemos remover os dados com segurança de não introduzir bias no modelo. Já no caso de **falta não aleatória**, não podemos simplesmente remover os dados faltantes, pois eles foram introduzidos sistematicamente, e a sua remoção pode fazer o modelo se comportar de forma inadequada em relação à realidade (pode gerar bias).\n",
    "\n",
    "Estas são algumas estratégias comumente adotadas para lidar com missing values:\n",
    "\n",
    "![handling_missing_data_summary](img/handling_missing_data_summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Listwise\n",
    "Remove todos os dados para um observação que tenha **pelo menos um missing value.**\n",
    "\n",
    "- Aplicável se poucas observações têm missing values.\n",
    "- Desvantajoso na maioria dos casos, pois a suposição de MCAR raramente é válida. Logo, a simples remoção dos missing values pode levar a estimações enviesadas.\n",
    "\n",
    "Em Python:\n",
    "\n",
    "`mydata.dropna(inplace=True)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pairwise\n",
    "Analisa os exemplos que têm valores não-nulos para as variáveis de interesse, no momento em que se deseja fazer um determinado cálculo.\n",
    "\n",
    "Veja o exemplo da imagem abaixo, em que se deseja calcular uma matriz de correlações:\n",
    "\n",
    "![corr_matrix](img/corr_matrix.png)\n",
    "\n",
    "Problema:\n",
    "- Cada correlação na matriz do caso exposto vai ser calculada usando uma quantidade diferente de exemplos.\n",
    "- Assume que os dados faltantes são MCAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dropping Variables\n",
    "Remover colunas que ultrapassem um threshold de missing values (p. ex.: >= 60%).\n",
    "\n",
    "Em Python:\n",
    "\n",
    "`del mydata.column_name`\n",
    "\n",
    "ou\n",
    "\n",
    "`mydata.drop('column_name', axis=1, inplace=True)`\n",
    "\n",
    "- Em geral, métodos de imputação costumam ser melhores do que métodos de remoção de features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Time-Series methods\n",
    "\n",
    "#### 1.4.1 Last Observation Carried Forward (LOCF) & Next Observation Carried Backward (NOCB)\n",
    "\n",
    "- LOCF: O último ponto registrado da série (o mais recente) é atribuído ao ponto faltante.\n",
    "\n",
    "- NOCB: O próximo ponto registrado da série é atribuído ao ponto faltante.\n",
    "\n",
    "Ex.:\n",
    "\n",
    "series = 3, 4, 5, 6, NA, 7, 8\n",
    "\n",
    "LOCF(series) = 3, 4, 5, 6, **6**, 7, 8\n",
    "\n",
    "NOCB(series) = 3, 4, 5, 6, **7**, 7, 8\n",
    "\n",
    "#### 1.4.2 Linear Interpolation\n",
    "Faz uma interpolação linear na variável com dados faltantes.\n",
    "\n",
    "Problema:\n",
    "- Pode não funcionar bem com dados sazonais, pois vai distorcer o comportamento habitual dos dados em períodos com características próprias.\n",
    "\n",
    "#### 1.4.3 Seasonal Adjustement + Linear Interpolation\n",
    "\n",
    "Tenta alinhar tendências de temporada com a interpolação.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Média, Mediana, Moda\n",
    "\n",
    "Imputar a média, a mediana ou a moda do campo com nulos.\n",
    "\n",
    "- Desvantagem: Pode reduzir a variância do dataset\n",
    "\n",
    "### 1.6 Por regressão linear\n",
    "\n",
    "Utilizamos um campo com valor faltando e usamos como target, para treinar uma regressão linear.\n",
    "\n",
    "Onde tem missing values, colocamos no dataset de teste. Onde tem valor para a target, usamos como treino.\n",
    "\n",
    "Assim, conseguimos aproximar o valor dos missing values por um modelo.\n",
    "\n",
    "- Desvantagens:\n",
    "- Overfitting (os valores aprendidos utilizaram as features do dataset, pode levar a resultados ruins quando utilizamos a feature “aprendida” em um outro modelo).\n",
    "- Supõe relação linear com a feature alvo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Multiple imputation\n",
    "\n",
    "Consistem em imputar o dataset com missings $m$ vezes, com valores provenientes de uma distribuição. Utiliza-se a abordagem da simulação de Markov Chain Monte Carlo (MCMC). Assim, obtemos $m$ datasets completos, os quais reunimos em um só, fazendo a média dos missings imputados de cada dataset, por exemplo.\n",
    "\n",
    "![multiple_imputation](img/multiple_imputation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tratamento de Outliers\n",
    "**Outliers** são valores extremos que destoam do restante dos dados.\n",
    "\n",
    "### 2.1 Tipos de Outliers\n",
    "Por quantidade de features envolvidas:\n",
    "- Univariados (relativos a uma única feature)\n",
    "- Multivariados (relativos a mais de uma feature)\n",
    "\n",
    "Por ambiente:\n",
    "- Point outliers: pontos isolados que se encontram longe do restante da distribuição\n",
    "- Contextual outliers: ruído nos dados, como símbolos de pontuação em análises de texto ou barulho de fundo em reconhecimento de voz\n",
    "- Collective outliers: pontos que representam novidades \n",
    "\n",
    "### 2.2 Causas\n",
    "As principais causas para a ocorrência de outliers são:\n",
    "\n",
    "- Erros de entrada de dados (erros humanos)\n",
    "- Erros de medida (erros instrumentais)\n",
    "- Erros experimentais (extração de dados)\n",
    "- Intencionais (dummy outliers para testar métodos de detecção)\n",
    "- Erros de processamento de dados (manipulação de dados)\n",
    "- Erros de amostragem (extração ou mistura de dados de fontes erradas ou diversas)\n",
    "- Naturais (não são erros, apenas novidades)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Detecção de outliers\n",
    "### Z-Score\n",
    "O z-score de uma observação é uma métrica que indica a quantos desvios padrões um ponto está da média da amostra (assumindo uma distribuição gaussiana): $z = \\frac{x - \\mu}{\\sigma}$. Valores comumente utilizados são:  2.5, 3 ou 3.5.\n",
    "\n",
    "OBS: frequentemente os dados não são descritos por uma distribuição gaussiana. Esse problema pode ser solucionado aplicando-se algum tipo de transformação nos dados (ex: scaling).\n",
    "\n",
    "#### DBScan\n",
    "O algoritmo de clusterização DBScan pode ser usado para detectar outliers (pontos que não se encontram na vizinhança de raio $eps$ de um **core point**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolation Forests\n",
    "O princípio básico das Isolation Forests é o de que outliers são **poucos e distantes do restante das observações**.\n",
    "\n",
    "Para isolar os outliers, uma feature é selecionada do conjunto de features aleatoriamente. Em seguida, é feita uma partição aleatória entre o valor máximo e mínimo para aquela feature. Através dessa partição aleatória, é possível detectar os outliers, que provavelmente estarão mais próximos da raiz da árvore (terão um “path length” menor do que os outros).\n",
    "\n",
    "- Aplicado para problema de **detecção de anomalias**\n",
    "- Vantagem em relação à construção de um perfil: ao invés de tentar construir um modelo de instâncias normais, esse método explicitamente isola pontos anômalos no dataset.\n",
    "\n",
    "![isolation_forest](img/isolation_forest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Categorização de Variáveis Contínuas e Discretas\n",
    "### 3.1 Label Encoding\n",
    "Se as variáveis são ordenadas (ex: \"best\" > \"good\" > \"fair\" > \"bad\") uma boa solução para transformá-las em categorias é simplesmente substituí-las por números. Ex: best = 2, good = 1, fair = 0, bad = -1.\n",
    "\n",
    "### 3.2 Dummy Encoding (One-Hot)\n",
    "Criar uma nova feature para cada valor que a variável pode assumir.\n",
    "\n",
    "#### Quando usar\n",
    "- Problemas com poucos “níveis”\n",
    "- Valores da variável não ordenados (ex.: nomes de países, nomes de estados, ...)\n",
    "- Modelos lineares (com qualquer quantidade de níveis)\n",
    "- Problemas em que o número de níveis é conhecido (é pouco provável surgirem novos)\n",
    "\n",
    "#### Quando não usar\n",
    "Modelos de árvore não lidam bem com one-hot-encodings de muitos níveis. Geralmente, essas features acabam sendo ignoradas no processo de construção da árvore.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Trap\n",
    "**Excluir uma das colunas geradas pelo One-Hot-Encoding.**\n",
    "O argumento para isso é estatístico: se não excluirmos uma das colunas, a soma dessas colunas será 1 para todas as linhas. Isso impacta a interpretabilidade dos coeficientes.\n",
    "\n",
    "Ex: modelo que fornece o custo de se alimentar determinado animal.\n",
    "\n",
    "$cost = 10*Species_{Cat} + 15*Species_{Dog} + 8*Species_{Rabbit} + 5*Species_{Snake}$\n",
    "\n",
    "Podemos escrever isso de outra forma, começando com um custo = 6 e reduzindo os valores dos coeficientes, como mostrado abaixo:\n",
    "\n",
    "$cost = 6 + 4*Species_{Cat} + 9*Species_{Dog} + 2*Species_{Rabbit} + (-1)*Species_{Snake}$\n",
    "\n",
    "Se tiramos um dos animais (ex: gato), o valor do intercept terá que ser obrigatoriamente 10, o que acaba fixando os outros coeficientes também:\n",
    "\n",
    "$cost = 10 + 5*Species_{Dog} + (-2)*Species_{Rabbit} + (-5)*Species_{Snake}$\n",
    "\n",
    "Na prática, muitas pessoas ignoram a dummy trap pois:\n",
    "- Introduz um viés em relação à variável excluída durante a regularização. Olhando no exemplo anterior, o custo relativo a gatos foi incorporado pelo intercept, que não sofre regularização (enquanto os coeficientes dos outros animais, sim).\n",
    "- Dificulta o tratamento de novos níveis. \n",
    "\n",
    "OBS: não usar pd.get_dummies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PCA\n",
    "Quando lidamos com um grande número de variáveis correlacionadas, os **componentes principais** permitem resumir esses dados a partir de um menor número de variáveis representativas, que coletivamente conseguem explicar a maior parte da variabilidade do dataset original. \n",
    "\n",
    "As direções dos componentes principais são aquelas ao longo das quais os dados originais apresentam **grande variabilidade**.\n",
    "\n",
    "**PCA** (Principal Component Analysis) é o processo que permite computar os **componentes principais**. Em outras palavras, o PCA permite encontrar uma representação de menor dimensionalidade do dataset que é capaz de explicar a maior parte da variabilidade original.  Trata-se de um método **não-supervisionado**, uma vez que envolve apenas as features $X_1, …, X_n$, e não uma variável resposta $Y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Principal components\n",
    "Cada um dos componentes principais é uma combinação linear de $p$ features. \n",
    "\n",
    "#### Primeiro componente\n",
    "O primeiro componente principal de um conjunto de features $X_1, X_2, …, X_p$ é a combinação linear normalizada dessas features que contém a maior variância.\n",
    "\n",
    "Podemos escrever esse componente como:\n",
    "\n",
    "$Z_1 = \\phi_{11}X_1, \\phi_{21}X_2, …, \\phi_{p1}X_p$\n",
    "\n",
    "Onde $\\sum_{j=1}^p\\phi_{j1}^2 = 1$. Os elementos $\\phi_{11}, .., \\phi_{p1}$ são conhecidos como **loadings** do primeiro componente principal, e $\\phi_1 = (\\phi_{11}, .., \\phi_{p1})^T$ é o **loading vector**. \n",
    "O loading vector do primeiro componente pode ser interpretado como sendo a linha em um espaço $p$-dimensional que é mais próxima das $n$ observações.\n",
    "\n",
    "Considerando que temos $n$ observações, o dataset $X$ será uma matriz $n x p$. Como o único aspecto que interessa em PCA é a variância, as variáveis são centralizadas de forma a terem média 0. Em seguida, é calculado:\n",
    "\n",
    "$z_{i1} = \\phi_{11}x_{i1}, \\phi_{21}x_{i2}, …, \\phi_{p1}x_{ip}$\n",
    "\n",
    "O problema de otimização do primeiro componente principal pode então ser escrito como:\n",
    "\n",
    "$\\text{maximize}_{ \\phi_{11}, ... ,  \\phi_{p1}} \\{ \\frac{1}{N}\\sum_{i=1}^n (\\sum_{j=1}^p \\phi_{j1}x_{ij})^2 \\} = \\text{maximize}_{ \\phi_{11}, ... ,  \\phi_{p1}} \\{ \\frac{1}{N}\\sum_{i=1}^n z_{i1}^2 \\}$, sujeito a $\\sum_{j=1}^p\\phi_{j1}^2 = 1$.\n",
    "\n",
    "$z_{11}, …, z_{n1}$ são chamados de **scores** do primeiro componente principal.\n",
    "\n",
    "#### Segundo componente\n",
    "O segundo componente é a combinação linear de $X_1, …, X_p$ que tem a maior variância entre todas as combinações lineares **não-correlacionadas com o primeiro componente principal**. Sendo $Z_1$ o primeiro componente principal e $Z_2$ o segundo, tem-se que $\\phi_1$ é ortogonal a $\\phi_2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Visualizando PCA\n",
    "Na figura abaixo, foi utilizado PCA no dataset de aprisionamentos nos EUA. Para cada um dos estados, o dataset possui a taxa (a cada 100 mil habitantes) para 3 tipos de crime (Assault, Murder e Rape), bem como a população UrbanPop.\n",
    "\n",
    "O plot abaixo, chamado de **biplot** mostra os scores dos componentes principais (em azul) e os loading vectors (em laranja). \n",
    "\n",
    "Ex: o loading para Rape no primeiro componente é 0.54 e no segundo componente é 0.17. Observa-se que o primeiro componente atribui aproximadamente o mesmo peso para Rape, Assault e Murder, que é muito maior do que o peso atribuído para UrbanPop. Esse componente pode ser interpretado como um resumo das taxas de crimes. \n",
    "\n",
    "Já o segundo componente atribui um peso maior a UrbanPop que aos crimes, podendo ser interpretado como uma taxa de urbanização do estado.\n",
    "\n",
    "![pca_components](img/pca_components.png)\n",
    "\n",
    "Um outro exemplo é mostrado abaixo. Nesse caso, o dataset é tridimensional.\n",
    "\n",
    "![pca_components_3d](img/pca_components_3d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Outros detalhes\n",
    "#### Scaling\n",
    "Antes de realizar PCA, é importante **centralizar os dados** para que eles tenham média zero. Além disso, os resultados obtidos com PCA irão depender se as variáveis foram individualmente redimensionadas (cada uma multiplicada por uma constante diferente).\n",
    "\n",
    "![pca_normalization](img/pca_normalization.png)\n",
    "\n",
    "O mais recomendado é redimensionar cada variável de forma que todas tenham um **desvio padrão unitário**.\n",
    "\n",
    "#### Visualização de dados\n",
    "O método PCA também é util para a visualização de dados, pois permite reduzir os dados para dimensões representaveis visualmente (2D ou 3D). O unico problema, é que as features no novo espaço não têm um significado tão claro, visto que são combinações lineares das variaveis do problema no espaço original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlação / Associação entre Dados Contínuos e entre Dados Discretos\n",
    "\n",
    "### 5.1 Covariância\n",
    "Covariância é a medida do quanto duas variáveis aleatórias mudam juntas.\n",
    "Ex: Peso e altura de girafas possuem covariância positiva, pois quando uma dessas variáveis é grande, a outra tende a ser grande também.\n",
    "\n",
    "Sejam $X$ e $Y$ duas variáveis aleatórias com médias $\\mu_X$ e $\\mu_Y$. A covariância de $X$ e $Y$ é definida como $cov(X,Y) = E[(X-\\mu_X)(Y-\\mu_Y)] = E[XY] - \\mu_X\\mu_Y$.\n",
    "- $cov(X,X) = var(X)$,\n",
    "- Se $X$ e $Y$ são independentes, $cov(X,Y) = 0$.\n",
    "\n",
    "Ex: Jogamos uma moeda justa 3 vezes. Seja $X$ o número de caras nas primeiras duas jogadas, e $Y$ o número de caras nas últimas 2 jogadas. Computar $cov(X,Y)$\n",
    "\n",
    "Com 3 jogadas, temos 8 resultados possíveis:\n",
    "HHH, HHT, HTH, HTT, THH, THT, TTH, TTT (H = cara, T = coroa)\n",
    "\n",
    "| X     \\      Y | 0         | 1              | 2         |\n",
    "|----------------|-----------|----------------|-----------|\n",
    "| 0              | TTT (1/8) | TTH (1/8)      | - (0)     |\n",
    "| 1              | HTT (1/8) | HTH, THT (1/4) | THH (1/8) |\n",
    "| 2              | - (0)     | HHT (1/8)      | HHH (1/8) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(X=0) = p(X=2) = 1/4$\n",
    "\n",
    "$p(X=1) = 1/2$\n",
    "\n",
    "$E(X) = E(Y) = 0*¼ + 2*¼ + 1*½ = 1$\n",
    "\n",
    "$cov(X, Y) = \\frac{1}{8}*(0 - 1)(0 - 1) + \\frac{1}{8}*(0 - 1)(1 - 1) + 0*(0 - 1)(2 - 1) +\n",
    "\\frac{1}{8}*(1-1)(0-1) + \\frac{1}{4}*(1-1)(1-1) + \\frac{1}{8}*(1-1)(2-1) +\n",
    "0*(2-1)(0-1) + \\frac{1}{8}*(2-1)(1-1) + \\frac{1}{8}*(2-1)(2-1) = \\frac{1}{4}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Correlação\n",
    "As unidades de covariância são **unidades de X vezes unidades de Y**, o que dificulta a comparação de covariâncias. A correlação é uma forma de unificar as escalas das covariâncias:\n",
    "\n",
    "$cor(X,Y) = \\frac{cov(X,Y)}{\\sigma_X\\sigma_Y}$\n",
    "\n",
    "- $-1 \\leq cor(X,Y) \\leq 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bases Desbalanceadas\n",
    "O que fazer quando as bases estão desbalanceadas:\n",
    "- Coletar mais dados\n",
    "- Mudar a métrica\n",
    "- Usar diferentes algoritmos\n",
    "\n",
    "### 6.1 Ajuste de amostragem\n",
    "#### Oversampling\n",
    "Consiste em **aumentar o número de observações da classe minoritária**. Para isso, é realizada uma amostragem com reposição.\n",
    "\n",
    "#### Undersampling\n",
    "Consiste em **diminuir o número de observações da classe majoritária**. Para isso, são usadas todas as observações da classe minoritária mais um conjunto aleatório de observações da classe majoritária.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

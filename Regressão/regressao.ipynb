{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão\n",
    "\n",
    "## 1 Regressão linear\n",
    "- Supervisionado\n",
    "- Linear\n",
    "- S imples x Multivariada\n",
    "- Paramétrico (assume um formato linear para o modelo)\n",
    "\n",
    "Equações:\n",
    "$h_{\\theta}(x_i) = \\theta_0 + \\theta_1x_1$\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x_i) - y_i)$\n",
    "\n",
    "### 1.1 Determinação dos parâmetros\n",
    "\n",
    "- Gradient Descent: $\\theta_j = \\theta_j - \\alpha\\frac{\\delta}{\\delta\\theta_j}J(\\theta)$\n",
    "    - Batch: todos os parâmetros usando todos os dados a cada iteração\n",
    "    - Mini batch: usar n dados a cada iteração\n",
    "    - Stochastic: todos os parâmetros usando uma linha por vez\n",
    "        - Quando o dataset é muito grande\n",
    "    - Least Squares <- J\n",
    "    - Learning rate:\n",
    "        - Pequeno: devagar\n",
    "        - Grande: ultrapassar o mínimo de 3 (divergindo)\n",
    "- Equação Normal: $\\theta = (X^TX)^{-1}X^Ty$\n",
    "    - Problema: calcular a inversa (O(n^3))\n",
    "- Maximum Likelihood:\n",
    "    - Baseia-se no produto das probabilidades de cada exemplo\n",
    "    - Chega em uma fórmula fechada para os estimadores dos parâmetros\n",
    "    - Problema: suposições na distribuição dos erros\n",
    "    - $y = a + bx + \\epsilon$: supomos que $\\epsilon ~ N(0, \\sigma^2)$. Logo $y ~ N(a+bx. \\sigma^2)$.\n",
    "        - $p_Y(y_i|x_i) = \\frac{1}{\\sqrt{s\\pi\\sigma^2}}e^{-\\frac{(y_i - (a+bx_i))^2}{2\\sigma^2}}$\n",
    "        - Objetivo: maximizar a log-verossimilhança $log(\\prod_{i=1}^np_Y(y_i|x_i))$\n",
    "\n",
    "**Precisa fazer Feature scaling!!** (NORMALIZAÇÃO, e não somente simples scaling)\n",
    "\n",
    "### 1.2 Regressão Linear Múltipla\n",
    "- Melhor fazer o modelo com todas as features do que fazer vários modelos individuais com cada feature\n",
    "- Remover variáveis correlacionadas (podem gerar modelos instáveis)\n",
    "\n",
    "### 1.3 R^2\n",
    "$R^2 = 1 - \\frac{RSS}{TSS}$\n",
    "\n",
    "Calcula a proporção da variância explicada pelo modelo.\n",
    "\n",
    "RSS: Residual Sum of Squares - variabilidade do output não explicada após fazer a regressão (minimizar)\n",
    "\n",
    "TSS: Total Sum of Squares - variabilidade do dataset\n",
    "\n",
    "[0, 1] => R^2= 1: grande parte da variabilidade é explicada pelo modelo\n",
    "\n",
    "Ao adicionar uma variável ao modelo ela sempre aumenta o R^2, se aumentar pouco, ela tem pouca contribuição pro modelo.\n",
    "(Se for muito ruim, pode dar negativo!)\n",
    "\n",
    "#### R^2 ajustado\n",
    "Penalização pela quantidade de features.\n",
    "\n",
    "R- assumes that every single variable explains the variation in the dependent variable. The adjusted R tells you the percentage of variation explained by only the independent variables that actually affect the dependent variable\n",
    "\n",
    "$R_{adj}^2 = 1 - \\frac{(1 - R^2)(n-1)}{n-k-1}$\n",
    "\n",
    "**RMSE, RMSLE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularização 11 e L2\n",
    "Redução do overfitting por meio da diminuição dos parâmetros theta.\n",
    "- Parâmetro de Regularização (Bias/Variance tradeoff)\n",
    "    - Muito grande, bias grande (underfit)\n",
    "    - Muito pequeno, variance grande (overfit) - caso original\n",
    "- Nunca regulariza $\\theta_0$ (somente os theta's associados a features)\n",
    "\n",
    "### 2.1 Regularização L2\n",
    "$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x_i) - y_i) + \\lambda\\sum_{j=1}^n\\theta_j^2$\n",
    "- Theta nunca vai pra zero (tende a zero)\n",
    "- Modelo linear: Ridge\n",
    "\n",
    "### 2.2 Regularização L1\n",
    "$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x_i) - y_i) + \\lambda\\sum_{j=1}^n|\\theta_j|$\n",
    "- $\\theta$'s podem ir pra zero: pode fazer feature selection\n",
    "- Modelo linear: Lasso\n",
    "\n",
    "OBS: Lasso + Ridge = ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Árvore de regressão\n",
    "- Supervisionado\n",
    "- Não Linear\n",
    "- Não Paramétrico\n",
    "\n",
    "**Vantagens:**\n",
    "- Fáceis de explicar\n",
    "- Podem ser mostradas visualmente\n",
    "- Não exigem feature scaling\n",
    "- Lida com variáveis qualitativas sem precisar de dummy variables\n",
    "- Mais robusto a outliers\n",
    "\n",
    "**Desvantagem**:\n",
    "- Alta variância (uma pequena modificação no dataset pode levar a uma série de splits diferentes) => melhorado por RF e Boosting\n",
    "    - Se a decision tree for feita muuuito complexa, com todas as decisões possíveis, ela terá overfitting\n",
    "\n",
    "**A ideia**:\n",
    "- Segmentar o espaço de tal forma que a predição de um novo exemplo será a média do exemplos de treino naquela região. (Minimizar o RSS)\n",
    "- Non overlapping regions\n",
    "\n",
    "### 3.1 Construção da árvore\n",
    "A cada etapa, usamos uma feature para dividir o dataset em duas regiões, escolhendo o threshold de tal forma que o RSS seja minimizado\n",
    "\n",
    "### 3.2 Pruning\n",
    "1. Partimos de uma árvore grande $T_0$, até que todos os nós terminais tenham no máximo um número n de observações (**Recursive Binary Splitting**)\n",
    "\n",
    "2. Criamos uma sequência de subárvores, em função de alpha:\n",
    "    \n",
    "    a. Para cada alpha existe uma subárvore T que minimiza:\n",
    "\n",
    "    $\\sum_{m=1^{|T|}}\\sum_{x_i \\in R_m}(y_i - \\hat{y}_{R_m})^2 + \\alpha|T|$\n",
    "\n",
    "Usamos K-fold CV para escolher o alpha que minimiza o MSE (**Cost Complexity Pruning**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Análise de residuos\n",
    "Assume premissas sobre os erros:\n",
    "- $\\epsilon_i$ e $\\epsilon_j$ são independentes ($i \\neq j$)\n",
    "- $Var(\\epsilon_i) = \\sigma^2$ (constante)\n",
    "- $\\epsilon_i \\approx N(0,\\sigma^2)$ (normalidade)\n",
    "- Modelo é linear\n",
    "- Não existem outliers influentes\n",
    "\n",
    "A ideia básica da análise dos residuos é que, se o modelo for apropriado, os residuos devem refletir as propriedades impostas pelo termo de erro do modelo.\n",
    "\n",
    "Serve para:\n",
    "- Ver o quão bem os dados se adequam ao modelo linear\n",
    "- Ver se o modelo que estamos usando é apropriado (plot de resíduos pode revelar que o modelo não é o melhor)\n",
    "\n",
    "### 4.1 Plot de Residuos\n",
    "- Gráfico de residuos ($e_i = y_i - \\hat{y}_i$) versus a variável dependente ($y$)\n",
    "- Pode ser usado para:\n",
    "\n",
    "\n",
    "#### 1. Identificar não-linearidade\n",
    "- Solução: transformar a variável independente (com log X, raiz de X...)\n",
    "    \n",
    "![nonlinear_residual](img/nonlinear_residual.png)\n",
    "\n",
    "#### 2. Verificar correlação entre erros numa série temporal\n",
    "\n",
    "#### 3. Checar variância não constante em erros (plot em formato de funil)\n",
    "- Heterocedasticidade\n",
    "- Solução: transformar a variável dependente (com log Y, raiz de Y...)\n",
    "\n",
    "![heterocedasticidade](img/heterocedasticidade.jpg)\n",
    "\n",
    "\n",
    "####  4. Detectar outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. KNN para Regressão\n",
    "- Supervisionado\n",
    "- Não paramétrico (sem suposições no decision boundary)\n",
    "- Preguiçoso\n",
    "- Precisa de feature scaling (pois é baseado na distância): se não alguma variável pode ser desprezada no cálculo da distância\n",
    "- Curse of dimensionality\n",
    "\n",
    "**Como funciona?**\n",
    "\n",
    "Cada novo exemplo recebe o valor da média do dos k vizinhos mais próximos\n",
    "\n",
    "**Diferentes distâncias:**\n",
    "- Euclidiana\n",
    "- Manhattan\n",
    "- *Cosseno (NLP)\n",
    "- Minkowski\n",
    "- Hamming (variáveis categóricas)\n",
    "\n",
    "**Como escolher K:**\n",
    "- K=1: overfit\n",
    "- K grande: undecit\n",
    "- Testar vários Ks e pegar o que minimiza o erro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "- Feature scaling: não muda a distribuição dos dados\n",
    "- Normalização: aproxima pela normal mais próxima\n",
    "\n",
    "Três tecnicas de padronização:\n",
    "### 1. Escalonamento padrão\n",
    "Subtração da média e divisão pelo desvio padrão: $\\frac{X-\\mu}{\\sigma}$\n",
    "- Outlier influencia fortemente no posicionamento da média\n",
    "- Menor poder de discriminação para dados nominais (não-outliers)\n",
    "\n",
    "### 2. Padronização para um intervalo [0,1]\n",
    "Escalonamento no intervalo [0,1]: $\\frac{(X - min(X)}{max(X) = min(X)}$\n",
    "- Outlier desloca os outros valores para perto de 0\n",
    "- Perigo: preditores podem passar a ignorar dados normais\n",
    "\n",
    "### 3. Escalonamento robusto\n",
    "Subtração da mediana e divisão pelo intervalo interquartil: $\\frac{X-\\bar{X}}{Range}$\n",
    "- Maior poder de discriminação\n",
    "- Outlier continua destoando dos dados normais"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
